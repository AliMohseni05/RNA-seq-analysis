{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDUCnWqxA5Vo",
        "outputId": "e5d326b3-c58b-4258-f173-585fe8e403e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,730 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,968 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Fetched 32.0 MB in 5s (6,720 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUa-8Az47Vq",
        "outputId": "cd293ee3-1089-4ace-bb2c-e4b8dddc1df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fastqc is already the newest version (0.11.9+dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "sra-toolkit is already the newest version (2.11.3+dfsg-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install  -y fastqc\n",
        "!apt-get install -y sra-toolkit\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SRA file (replace SRRXXXXXXX with your SRA run accession)\n",
        "#!prefetch SRRXXXXXXX\n",
        "\n",
        "# Convert to FASTQ (single-end example)\n",
        "#!fastq-dump --split-files SRRXXXXXXX.sra\n"
      ],
      "metadata": {
        "id": "W9BwlDzh5vaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SRA file (replace SRRXXXXXXX with your SRA run accession)\n",
        "!prefetch SRR9879594\n",
        "\n",
        "# Convert to FASTQ (single-end example)\n",
        "#!fastq-dump --split-files SRRXXXXXXX.sra\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghJIv7I2RvTa",
        "outputId": "6c07e70e-7811-4748-fe61-eacc61f13ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2025-05-25T07:08:27 prefetch.2.11.3: Current preference is set to retrieve SRA Normalized Format files with full base quality scores.\n",
            "2025-05-25T07:08:27 prefetch.2.11.3: 1) Downloading 'SRR9879594'...\n",
            "2025-05-25T07:08:27 prefetch.2.11.3: SRA Normalized Format file is being retrieved, if this is different from your preference, it may be due to current file availability.\n",
            "2025-05-25T07:08:27 prefetch.2.11.3:  Downloading via HTTPS...\n",
            "2025-05-25T07:08:45 prefetch.2.11.3:  HTTPS download succeed\n",
            "2025-05-25T07:08:50 prefetch.2.11.3:  'SRR9879594' is valid\n",
            "2025-05-25T07:08:50 prefetch.2.11.3: 1) 'SRR9879594' was downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def download_sra_to_fastq(sra_accession, split_files=True, max_spots=None):\n",
        "    \"\"\"\n",
        "    Download SRA data and convert to FASTQ using SRA Toolkit commands.\n",
        "\n",
        "    Args:\n",
        "        sra_accession (str): Valid SRA run accession (e.g., 'SRR11605094')\n",
        "        split_files (bool): Split paired-end reads into separate files\n",
        "        max_spots (int or None): Limit number of spots to download\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate accession format\n",
        "        if not sra_accession.startswith(('SRR', 'ERR', 'DRR')):\n",
        "            raise ValueError(\"Accession must start with SRR, ERR, or DRR\")\n",
        "\n",
        "        # Create SRA directory structure\n",
        "        sra_dir = Path.home() / \"ncbi\" / \"public\" / \"sra\"\n",
        "        sra_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 1. Download with prefetch (with retries)\n",
        "        print(f\"ðŸ“¥ Downloading {sra_accession}...\")\n",
        "        for attempt in range(3):  # Retry up to 3 times\n",
        "            try:\n",
        "                subprocess.run(\n",
        "                    ['prefetch', sra_accession, '-O', str(sra_dir)],\n",
        "                    check=True,\n",
        "                    stderr=subprocess.PIPE\n",
        "                )\n",
        "                break\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                if attempt == 2:\n",
        "                    raise\n",
        "                print(f\"âš ï¸ Retry {attempt + 1}/3...\")\n",
        "\n",
        "        # 2. Verify download\n",
        "        sra_path = sra_dir / f\"{sra_accession}.sra\"\n",
        "        if not sra_path.exists():\n",
        "            raise FileNotFoundError(f\"SRA file not found at {sra_path}\")\n",
        "\n",
        "        # 3. Convert to FASTQ\n",
        "        print(f\"âš™ï¸ Converting to FASTQ...\")\n",
        "        fastq_cmd = [\n",
        "            'fasterq-dump',\n",
        "            '--outdir', '.',\n",
        "            '--threads', '4',\n",
        "            '--verbose'\n",
        "        ]\n",
        "\n",
        "        if split_files:\n",
        "            fastq_cmd.append('--split-files')\n",
        "        if max_spots:\n",
        "            fastq_cmd.extend(['--max-spot-id', str(max_spots)])\n",
        "\n",
        "        fastq_cmd.append(str(sra_path))\n",
        "\n",
        "        subprocess.run(fastq_cmd, check=True)\n",
        "\n",
        "        # 4. Cleanup\n",
        "        sra_path.unlink()\n",
        "        print(f\"âœ… Successfully processed {sra_accession}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to process {sra_accession}: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "rOvaKFdoQBdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "def download_rnaseq_sra(srr_id, out_dir='.'):\n",
        "    \"\"\"\n",
        "    Download RNA-seq data from SRA using an SRR code.\n",
        "    Requires: SRA Toolkit installed and in PATH.\n",
        "\n",
        "    Args:\n",
        "        srr_id (str): The SRR accession code (e.g., 'SRR9879594').\n",
        "        out_dir (str): Directory to save the downloaded files.\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Step 1: Download the .sra file using prefetch\n",
        "    print(f\"Downloading SRA file for {srr_id}...\")\n",
        "    subprocess.run(['prefetch', srr_id, '--output-directory', out_dir], check=True)\n",
        "\n",
        "    # Step 2: Convert .sra to .fastq using fasterq-dump\n",
        "    sra_path = os.path.join(out_dir, srr_id)\n",
        "    print(f\"Converting {srr_id} to FASTQ...\")\n",
        "    subprocess.run(['fasterq-dump', sra_path, '-O', out_dir], check=True)\n",
        "\n",
        "    print(f\"Download and conversion complete. Files saved in {out_dir}\")\n",
        "\n",
        "# Example usage:\n",
        "# download_rnaseq_sra('SRR9879594', out_dir='./rna_seq_data')\n"
      ],
      "metadata": {
        "id": "uaZ3bod6Uxft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#donwload RNA-seq data"
      ],
      "metadata": {
        "id": "qaBW_8VrLFu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_sra=[]\n",
        "num=9879602\n",
        "for i in range(4):\n",
        "  ssr=\"SRR\"\n",
        "  srrt= ssr+str(num)\n",
        "  list_sra.append(srrt)\n",
        "  num+=1\n",
        "\n",
        "print(list_sra)\n",
        "for srr_id in list_sra:\n",
        "  download_rnaseq_sra(srr_id, out_dir='sra-tomato-4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qApnEXOyMJIO",
        "outputId": "d6d33142-5628-4b79-879d-eb62e8fda724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SRR9879602', 'SRR9879603', 'SRR9879604', 'SRR9879605']\n",
            "Downloading SRA file for SRR9879602...\n",
            "Converting SRR9879602 to FASTQ...\n",
            "Download and conversion complete. Files saved in sra-tomato-4\n",
            "Downloading SRA file for SRR9879603...\n",
            "Converting SRR9879603 to FASTQ...\n",
            "Download and conversion complete. Files saved in sra-tomato-4\n",
            "Downloading SRA file for SRR9879604...\n",
            "Converting SRR9879604 to FASTQ...\n",
            "Download and conversion complete. Files saved in sra-tomato-4\n",
            "Downloading SRA file for SRR9879605...\n",
            "Converting SRR9879605 to FASTQ...\n",
            "Download and conversion complete. Files saved in sra-tomato-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def download_sra_to_fastq(sra_accession, split_files=True, max_spots=None):\n",
        "    \"\"\"\n",
        "    Download SRA data and convert to FASTQ using SRA Toolkit commands.\n",
        "\n",
        "    Args:\n",
        "        sra_accession (str): The SRA run accession (e.g., 'SRR1234567').\n",
        "        split_files (bool): Whether to split paired-end reads into separate files.\n",
        "        max_spots (int or None): Limit number of reads downloaded (None = no limit).\n",
        "\n",
        "    Returns:\n",
        "        None. Downloads files to current working directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Install SRA Toolkit (skip if already installed)\n",
        "        print(\"Installing SRA Toolkit...\")\n",
        "        subprocess.run(['apt-get', 'update'], check=True)\n",
        "        subprocess.run(['apt-get', 'install', '-y', 'sra-toolkit'], check=True)\n",
        "\n",
        "        # Download SRA file using prefetch\n",
        "        print(f\"Downloading SRA file for {sra_accession}...\")\n",
        "        subprocess.run(['prefetch', sra_accession], check=True)\n",
        "\n",
        "        # Build fastq-dump command\n",
        "        fastq_cmd = ['fastq-dump']\n",
        "        if split_files:\n",
        "            fastq_cmd.append('--split-files')\n",
        "        if max_spots is not None:\n",
        "            fastq_cmd.extend(['--maxSpotId', str(max_spots)])\n",
        "        fastq_cmd.append(f'{sra_accession}.sra')\n",
        "\n",
        "        # Convert SRA to FASTQ\n",
        "        print(f\"Converting {sra_accession}.sra to FASTQ...\")\n",
        "        subprocess.run(fastq_cmd, check=True)\n",
        "\n",
        "        print(\"Download and conversion completed successfully.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "download_sra_to_fastq('SRP217045', split_files=True, max_spots=100000)\n"
      ],
      "metadata": {
        "id": "rM561OsJBkEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5546f24-56e7-4561-e1bd-9d3c97661737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing SRA Toolkit...\n",
            "Downloading SRA file for SRP217045...\n",
            "Error occurred: Command '['prefetch', 'SRP217045']' returned non-zero exit status 64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fastqc /content/sra-tomato-4/SRR9879602.fastq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpKGPZsgOXWf",
        "outputId": "d2733edd-1a02-4be3-f685-19cbf70c837a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started analysis of SRR9879602.fastq\n",
            "Approx 5% complete for SRR9879602.fastq\n",
            "Approx 10% complete for SRR9879602.fastq\n",
            "Approx 15% complete for SRR9879602.fastq\n",
            "Approx 20% complete for SRR9879602.fastq\n",
            "Approx 25% complete for SRR9879602.fastq\n",
            "Approx 30% complete for SRR9879602.fastq\n",
            "Approx 35% complete for SRR9879602.fastq\n",
            "Approx 40% complete for SRR9879602.fastq\n",
            "Approx 45% complete for SRR9879602.fastq\n",
            "Approx 50% complete for SRR9879602.fastq\n",
            "Approx 55% complete for SRR9879602.fastq\n",
            "Approx 60% complete for SRR9879602.fastq\n",
            "Approx 65% complete for SRR9879602.fastq\n",
            "Approx 70% complete for SRR9879602.fastq\n",
            "Approx 75% complete for SRR9879602.fastq\n",
            "Approx 80% complete for SRR9879602.fastq\n",
            "Approx 85% complete for SRR9879602.fastq\n",
            "Approx 90% complete for SRR9879602.fastq\n",
            "Approx 95% complete for SRR9879602.fastq\n",
            "Analysis complete for SRR9879602.fastq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fastp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQinxR91Q-Fm",
        "outputId": "36c9938e-0ab6-4dcd-b0ed-4f6c09cec71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fastp\n",
            "0 upgraded, 1 newly installed, 0 to remove and 91 not upgraded.\n",
            "Need to get 193 kB of archives.\n",
            "After this operation, 640 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fastp amd64 0.20.1+dfsg-1 [193 kB]\n",
            "Fetched 193 kB in 0s (598 kB/s)\n",
            "Selecting previously unselected package fastp.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../fastp_0.20.1+dfsg-1_amd64.deb ...\n",
            "Unpacking fastp (0.20.1+dfsg-1) ...\n",
            "Setting up fastp (0.20.1+dfsg-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim single-end FASTQ file\n",
        "!fastp -i input.fastq -o trimmed_output.fastq -h fastp_report.html -j fastp_report.json"
      ],
      "metadata": {
        "id": "0MD5I7X-SaO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d909cd-8325-4995-bd60-b07401db5bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fastp: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim single-end FASTQ file\n",
        "!fastp -i /content/SRR2016724_1.fastq -o SRR2016724_1.fastq -h fastp_report.html -j fastp_report.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvp_3LwdRHk2",
        "outputId": "d3357d0b-113a-483d-ab5e-23401962d3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: fastp: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trim paired-end FASTQ files\n",
        "!fastp -i input_1.fastq -I input_2.fastq -o trimmed_1.fastq -O trimmed_2.fastq -h fastp_report.html -j fastp_report.json"
      ],
      "metadata": {
        "id": "kPsTHr8OSGHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget   https://ftp.ensembl.org/pub/release-114/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna.toplevel.fa.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lypzr1pDSSox",
        "outputId": "af685973-e4ee-4ef2-8c56-253b8f5d82dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-21 05:26:14--  https://ftp.ensembl.org/pub/release-114/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna.toplevel.fa.gz\n",
            "Resolving ftp.ensembl.org (ftp.ensembl.org)... 193.62.193.169\n",
            "Connecting to ftp.ensembl.org (ftp.ensembl.org)|193.62.193.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 806418890 (769M) [application/x-gzip]\n",
            "Saving to: â€˜Mus_musculus.GRCm39.dna.toplevel.fa.gzâ€™\n",
            "\n",
            "Mus_musculus.GRCm39 100%[===================>] 769.06M  31.4MB/s    in 26s     \n",
            "\n",
            "2025-05-21 05:26:40 (29.8 MB/s) - â€˜Mus_musculus.GRCm39.dna.toplevel.fa.gzâ€™ saved [806418890/806418890]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###tomato ref"
      ],
      "metadata": {
        "id": "FbvYdDXd5GtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/solanum_lycopersicum/dna/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA8OWdwdRYnp",
        "outputId": "a2c83b04-ee3a-41c0-c5a8-f941c1dc7639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-27 06:50:35--  https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/solanum_lycopersicum/dna/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz\n",
            "Resolving ftp.ensemblgenomes.ebi.ac.uk (ftp.ensemblgenomes.ebi.ac.uk)... 193.62.193.161\n",
            "Connecting to ftp.ensemblgenomes.ebi.ac.uk (ftp.ensemblgenomes.ebi.ac.uk)|193.62.193.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 227747063 (217M) [application/x-gzip]\n",
            "Saving to: â€˜Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gzâ€™\n",
            "\n",
            "Solanum_lycopersicu 100%[===================>] 217.20M  13.7MB/s    in 16s     \n",
            "\n",
            "2025-05-27 06:50:53 (13.2 MB/s) - â€˜Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gzâ€™ saved [227747063/227747063]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz"
      ],
      "metadata": {
        "id": "B-LLUVtshFza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBTcdxEhWvQg",
        "outputId": "171fff46-042e-4039-9b4b-328e5f341014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloud.biohpc.swmed.edu/index.php/s/hisat2-220-download/download -O hisat2.zip\n",
        "!unzip hisat2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X93aHa-2Vviy",
        "outputId": "62e2fdd9-e3a6-47cb-8d9f-bf5d5c3d808d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 05:20:53--  https://cloud.biohpc.swmed.edu/index.php/s/hisat2-220-download/download\n",
            "Resolving cloud.biohpc.swmed.edu (cloud.biohpc.swmed.edu)... 129.112.9.92\n",
            "Connecting to cloud.biohpc.swmed.edu (cloud.biohpc.swmed.edu)|129.112.9.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-25 05:20:53 ERROR 404: Not Found.\n",
            "\n",
            "Archive:  hisat2.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of hisat2.zip or\n",
            "        hisat2.zip.zip, and cannot find hisat2.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c bioconda hisat2 -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fVwVGkYXJl_",
        "outputId": "3bd8f909-6ae3-479a-abbc-3d875687ceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - bioconda\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - hisat2\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2025.4.26  |       hbd8a1cb_0         149 KB  conda-forge\n",
            "    certifi-2025.4.26          |     pyhd8ed1ab_0         154 KB  conda-forge\n",
            "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
            "    hisat2-2.2.1               |       h503566f_8        15.9 MB  bioconda\n",
            "    openssl-3.5.0              |       h7b32b05_1         3.0 MB  conda-forge\n",
            "    perl-5.32.1                | 7_hd590300_perl5        12.7 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        33.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  hisat2             bioconda/linux-64::hisat2-2.2.1-h503566f_8 \n",
            "  perl               conda-forge/linux-64::perl-5.32.1-7_hd590300_perl5 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.4.26-hbd8a1cb_0 \n",
            "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2025.4.26-pyhd8ed1ab_0 \n",
            "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
            "  openssl                                  3.4.0-h7b32b05_1 --> 3.5.0-h7b32b05_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "hisat2-2.2.1         | 15.9 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "perl-5.32.1          | 12.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "certifi-2025.4.26    | 154 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hisat2-2.2.1         | 15.9 MB   | :   0% 0.0009822275799240931/1 [00:00<01:42, 102.90s/it]\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | :   2% 0.02102257964143311/1 [00:00<00:05,  5.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | :   1% 0.01364371451460054/1 [00:00<00:07,  7.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "perl-5.32.1          | 12.7 MB   | :   0% 0.0012277751453917628/1 [00:00<01:30, 90.65s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "certifi-2025.4.26    | 154 KB    | :  10% 0.1042239185750636/1 [00:00<00:00,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "hisat2-2.2.1         | 15.9 MB   | :  20% 0.20037442630451502/1 [00:00<00:00,  1.17it/s]   \n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  7.81s/it]                \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | :  11% 0.10758915965669182/1 [00:00<00:01,  1.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "perl-5.32.1          | 12.7 MB   | :  19% 0.19153292268111502/1 [00:00<00:00,  1.04it/s]  \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | : 100% 1.0/1 [00:00<00:00,  1.89s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "certifi-2025.4.26    | 154 KB    | : 100% 1.0/1 [00:00<00:00,  4.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "certifi-2025.4.26    | 154 KB    | : 100% 1.0/1 [00:00<00:00,  4.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.86it/s]                \u001b[A\u001b[A\n",
            "\n",
            "hisat2-2.2.1         | 15.9 MB   | :  43% 0.4341445903264492/1 [00:00<00:00,  1.65it/s] \n",
            "perl-5.32.1          | 12.7 MB   | :  52% 0.5242599870822827/1 [00:00<00:00,  1.95it/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 149 KB    | : 100% 1.0/1 [00:00<00:00,  3.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hisat2-2.2.1         | 15.9 MB   | :  67% 0.674790347407852/1 [00:00<00:00,  1.93it/s] \n",
            "perl-5.32.1          | 12.7 MB   | :  88% 0.8839981046820693/1 [00:00<00:00,  2.52it/s]\u001b[A\n",
            "hisat2-2.2.1         | 15.9 MB   | : 100% 1.0/1 [00:00<00:00,  1.00it/s]\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get -y install hisat2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXF9oxTSbaUz",
        "outputId": "8df2a813-3726-44f2-8562-15d063b36be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,729 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,969 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 32.0 MB in 5s (6,104 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  bcftools libhts3 libhtscodecs2 python3-hisat2 samtools\n",
            "Suggested packages:\n",
            "  python3-numpy python3-matplotlib texlive-latex-recommended cwltool\n",
            "The following NEW packages will be installed:\n",
            "  bcftools hisat2 libhts3 libhtscodecs2 python3-hisat2 samtools\n",
            "0 upgraded, 6 newly installed, 0 to remove and 100 not upgraded.\n",
            "Need to get 5,505 kB of archives.\n",
            "After this operation, 17.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhtscodecs2 amd64 1.1.1-3 [53.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhts3 amd64 1.13+ds-2build1 [390 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 bcftools amd64 1.13-1 [697 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 hisat2 amd64 2.2.1-3 [3,832 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-hisat2 all 2.2.1-3 [12.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 samtools amd64 1.13-4 [520 kB]\n",
            "Fetched 5,505 kB in 3s (1,837 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libhtscodecs2:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libhtscodecs2_1.1.1-3_amd64.deb ...\n",
            "Unpacking libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Selecting previously unselected package libhts3:amd64.\n",
            "Preparing to unpack .../1-libhts3_1.13+ds-2build1_amd64.deb ...\n",
            "Unpacking libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Selecting previously unselected package bcftools.\n",
            "Preparing to unpack .../2-bcftools_1.13-1_amd64.deb ...\n",
            "Unpacking bcftools (1.13-1) ...\n",
            "Selecting previously unselected package hisat2.\n",
            "Preparing to unpack .../3-hisat2_2.2.1-3_amd64.deb ...\n",
            "Unpacking hisat2 (2.2.1-3) ...\n",
            "Selecting previously unselected package python3-hisat2.\n",
            "Preparing to unpack .../4-python3-hisat2_2.2.1-3_all.deb ...\n",
            "Unpacking python3-hisat2 (2.2.1-3) ...\n",
            "Selecting previously unselected package samtools.\n",
            "Preparing to unpack .../5-samtools_1.13-4_amd64.deb ...\n",
            "Unpacking samtools (1.13-4) ...\n",
            "Setting up libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Setting up libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Setting up bcftools (1.13-1) ...\n",
            "Setting up samtools (1.13-4) ...\n",
            "Setting up hisat2 (2.2.1-3) ...\n",
            "Setting up python3-hisat2 (2.2.1-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# make ref indexs"
      ],
      "metadata": {
        "id": "hDLIdnbwnAp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz\n"
      ],
      "metadata": {
        "id": "xI629BqGdR0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hisat2-build -p 8 /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa reference_index\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQEfxK8edbmS",
        "outputId": "6f3f8327-0cab-4156-a12b-8901a44b28d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings:\n",
            "  Output files: \"reference_index.*.ht2\"\n",
            "  Line rate: 6 (line is 64 bytes)\n",
            "  Lines per side: 1 (side is 64 bytes)\n",
            "  Offset rate: 4 (one in 16)\n",
            "  FTable chars: 10\n",
            "  Strings: unpacked\n",
            "  Local offset rate: 3 (one in 8)\n",
            "  Local fTable chars: 6\n",
            "  Local sequence length: 57344\n",
            "  Local sequence overlap between two consecutive indexes: 1024\n",
            "  Endianness: little\n",
            "  Actual local endianness: little\n",
            "  Sanity checking: disabled\n",
            "  Assertions: disabled\n",
            "  Random seed: 0\n",
            "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
            "Input files DNA, FASTA:\n",
            "  /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa\n",
            "Reading reference sizes\n",
            "  Time reading reference sizes: 00:00:12\n",
            "Calculating joined length\n",
            "Writing header\n",
            "Reserving space for joined string\n",
            "Joining reference sequences\n",
            "  Time to join reference sequences: 00:00:07\n",
            "  Time to read SNPs and splice sites: 00:00:00\n",
            "Using parameters --bmax 17492390 --dcv 1024\n",
            "  Doing ahead-of-time memory usage test\n",
            "  Passed!  Constructing with these parameters: --bmax 17492390 --dcv 1024\n",
            "Constructing suffix-array element generator\n",
            "Building DifferenceCoverSample\n",
            "  Building sPrime\n",
            "  Building sPrimeOrder\n",
            "  V-Sorting samples\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#maping"
      ],
      "metadata": {
        "id": "mKQn4IZKsx7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#p\n",
        "!hisat2 -p 4 -x reference_index -1 reads_1.fastq -2 reads_2.fastq -S output.sam --summary-file summary.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhOdAxOrWLII",
        "outputId": "4d18ea0d-94c6-46f4-dfeb-c23b2346200a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: [Errno 2] No such file or directory: 'reads_1.fastq'\n",
            "(ERR): Read file 'reads_1.fastq' doesn't exist\n",
            "Exiting now ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#s\n",
        "!hisat2 -p 4 -x reference_index -U reads.fastq -S output.sam --summary-file summary.txt\n"
      ],
      "metadata": {
        "id": "mU98myzoWdh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abcf3c57-6abb-496f-f960-314e88c245ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: [Errno 2] No such file or directory: 'reads.fastq'\n",
            "(ERR): Read file 'reads.fastq' doesn't exist\n",
            "Exiting now ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir random_name"
      ],
      "metadata": {
        "id": "4w-Z19cMedYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92563615-447c-4806-ab17-ff6558a9a2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜random_nameâ€™: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz"
      ],
      "metadata": {
        "id": "5Dtd8NX8lrBG",
        "outputId": "f2dae816-ece0-4152-8d29-bcdad55eb4b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa.gz: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hisat2-build /content/Solanum_lycopersicum.SL3.0.dna.toplevel.fa reference_index"
      ],
      "metadata": {
        "id": "-c-W6AhgmCEq",
        "outputId": "ffd0dc9a-4a13-4c76-eb02-a21f2f4c69d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: hisat2-build: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sam bam"
      ],
      "metadata": {
        "id": "NN2IgaXpqhzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i have 8 index.ht2 file and 4 .fastq file i want to make .sam fille with hsat2\n",
        "\n",
        "import glob\n",
        "\n",
        "# Get a list of all fastq files in the current directory\n",
        "fastq_files = glob.glob('*.fastq')\n",
        "\n",
        "# Assuming paired-end reads and index files are in the same directory\n",
        "index_prefix = 'reference_index' # Replace with the actual prefix if different\n",
        "\n",
        "# Iterate through fastq files and run hisat2\n",
        "for fastq_file in fastq_files:\n",
        "    # Construct output SAM file name\n",
        "    output_sam = fastq_file.replace('.fastq', '.sam')\n",
        "\n",
        "    # Determine if it's a paired-end read (assuming _1.fastq and _2.fastq)\n",
        "    if '_1.fastq' in fastq_file:\n",
        "        read2_file = fastq_file.replace('_1.fastq', '_2.fastq')\n",
        "        if read2_file in fastq_files:\n",
        "            print(f\"Mapping paired-end reads: {fastq_file} and {read2_file} to {output_sam}\")\n",
        "            !hisat2 -p 4 -x {index_prefix} -1 {fastq_file} -2 {read2_file} -S {output_sam} --summary-file {output_sam.replace('.sam', '_summary.txt')}\n",
        "            # Remove the corresponding _2.fastq from the list to avoid double processing\n",
        "            fastq_files.remove(read2_file)\n",
        "        else:\n",
        "            print(f\"Warning: Found {fastq_file} but no matching paired-end file.\")\n",
        "            print(f\"Mapping single-end read: {fastq_file} to {output_sam}\")\n",
        "            !hisat2 -p 4 -x {index_prefix} -U {fastq_file} -S {output_sam} --summary-file {output_sam.replace('.sam', '_summary.txt')}\n",
        "    elif '_2.fastq' in fastq_file:\n",
        "         # This file will be processed when its _1.fastq counterpart is encountered\n",
        "         pass # Do nothing, it will be handled by the _1.fastq logic\n",
        "    else:\n",
        "        # Assume it's a single-end read if it doesn't have _1 or _2\n",
        "        print(f\"Mapping single-end read: {fastq_file} to {output_sam}\")\n",
        "        !hisat2 -p 4 -x {index_prefix} -U {fastq_file} -S {output_sam} --summary-file {output_sam.replace('.sam', '_summary.txt')}\n",
        "\n",
        "print(\"HISAT2 mapping complete for all fastq files.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-WYcL35LqhHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.ht2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQnp_NuxvVlP",
        "outputId": "5cc95eeb-857d-4f75-bf08-d4d819f83373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 242M May 25 05:52 reference_index.1.ht2\n",
            "-rw-r--r-- 1 root root 178M May 25 05:52 reference_index.2.ht2\n",
            "-rw-r--r-- 1 root root 201K May 25 05:24 reference_index.3.ht2\n",
            "-rw-r--r-- 1 root root 178M May 25 05:24 reference_index.4.ht2\n",
            "-rw-r--r-- 1 root root 341M May 25 05:54 reference_index.5.ht2\n",
            "-rw-r--r-- 1 root root 182M May 25 05:54 reference_index.6.ht2\n",
            "-rw-r--r-- 1 root root   12 May 25 05:25 reference_index.7.ht2\n",
            "-rw-r--r-- 1 root root    8 May 25 05:25 reference_index.8.ht2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p refgene\n",
        "!mv *.ht2 refgene"
      ],
      "metadata": {
        "id": "JncFjvHbxeEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hisat2 -x /content/reference_index/*.ht2 -U /content/sra-tomato-4/*.fastq -S combined_output.sam\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PThjXNIvcig",
        "outputId": "6371d2d4-f75d-4f16-c190-2c0610d2e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ERR): \"/content/reference_index/reference_index.1.ht2\" does not exist\n",
            "Exiting now ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_sra=[]\n",
        "num=9879602\n",
        "for i in range(4):\n",
        "  l=f\"SRR{num}.fastq\"\n",
        "  list_sra.append(l)\n",
        "  num+=1\n",
        "print(list_sra)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLGvXNzDLRZS",
        "outputId": "2250425a-7a9a-47e0-f36c-f0dbb742ad21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SRR9879602.fastq', 'SRR9879603.fastq', 'SRR9879604.fastq', 'SRR9879605.fastq']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hisat2 -x /content/reference_index/reference_index \\\n",
        "        -U /content/sra-tomato-4/SRR9879602.fastq\\\n",
        "        -S /content/combined_output.sam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPFM1vy5I5h_",
        "outputId": "4d00987d-bf24-436b-a66a-f557a404b385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17401320 reads; of these:\n",
            "  17401320 (100.00%) were unpaired; of these:\n",
            "    1936793 (11.13%) aligned 0 times\n",
            "    15274069 (87.78%) aligned exactly 1 time\n",
            "    190458 (1.09%) aligned >1 times\n",
            "88.87% overall alignment rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/combined_output.sam  # Verify SAM file was created\n",
        "!head /content/combined_output.sam    # Peek at alignment results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUccO77xRTdg",
        "outputId": "8cdefaed-9ccb-400a-9da4-a6d21c7a3b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.0G May 25 11:44 /content/combined_output.sam\n",
            "@HD\tVN:1.0\tSO:unsorted\n",
            "@SQ\tSN:1\tLN:98455869\n",
            "@SQ\tSN:2\tLN:55977580\n",
            "@SQ\tSN:3\tLN:72290146\n",
            "@SQ\tSN:4\tLN:66557038\n",
            "@SQ\tSN:5\tLN:66723567\n",
            "@SQ\tSN:6\tLN:49794276\n",
            "@SQ\tSN:7\tLN:68175699\n",
            "@SQ\tSN:8\tLN:65987440\n",
            "@SQ\tSN:9\tLN:72906345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!samtools view -b /content/combined_output.sam > /content/combined_output.bam"
      ],
      "metadata": {
        "id": "uIPDCQV_RavE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!# Sort BAM by genomic coordinates\n",
        "!samtools sort -@ 4 -o sample.sorted.bam /content/combined_output.bam\n",
        "\n",
        "# Index the sorted BAM\n",
        "!samtools index sample.sorted.bam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PclFuUPgHsP3",
        "outputId": "f468031e-846a-47f3-866c-b111d38cf2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bam_sort_core] merging from 4 files and 4 in-memory blocks...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HTSeq  # Install HTSeq Python package\n",
        "!htseq-count --help # Verify installation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2nyTM4iXj-x",
        "outputId": "6d56cca0-9284-4b4c-c508-a10ae10d11ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting HTSeq\n",
            "  Downloading HTSeq-2.0.9-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting numpy (from HTSeq)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pysam (from HTSeq)\n",
            "  Downloading pysam-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Downloading HTSeq-2.0.9-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysam-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (26.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pysam, numpy, HTSeq\n",
            "Successfully installed HTSeq-2.0.9 numpy-2.2.6 pysam-0.23.0\n",
            "usage: htseq-count [-h] [--version] [-f {sam,bam,auto}] [-r {pos,name}]\n",
            "                   [--max-reads-in-buffer MAX_BUFFER_SIZE]\n",
            "                   [-s {yes,no,reverse}] [-a MINAQUAL] [-t FEATURE_TYPE]\n",
            "                   [-i IDATTR] [--additional-attr ADDITIONAL_ATTRIBUTES]\n",
            "                   [--add-chromosome-info]\n",
            "                   [-m {union,intersection-strict,intersection-nonempty}]\n",
            "                   [--nonunique {none,all,fraction,random}]\n",
            "                   [--secondary-alignments {score,ignore}]\n",
            "                   [--supplementary-alignments {score,ignore}] [-o SAMOUTS]\n",
            "                   [-p {SAM,BAM,sam,bam}] [-d OUTPUT_DELIMITER]\n",
            "                   [-c OUTPUT_FILENAME] [--counts-output-sparse]\n",
            "                   [--append-output] [-n NPROCESSES]\n",
            "                   [--feature-query FEATURE_QUERY] [-q] [--with-header]\n",
            "                   samfilenames [samfilenames ...] featuresfilename\n",
            "\n",
            "This script takes one or more alignment files in SAM/BAM format and a feature\n",
            "file in GFF format and calculates for each feature the number of reads mapping\n",
            "to it. See http://htseq.readthedocs.io/en/master/count.html for details.\n",
            "\n",
            "positional arguments:\n",
            "  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n",
            "                        If '-' is selected, read from standard input\n",
            "  featuresfilename      Path to the GTF file containing the features\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --version             Show software version and exit\n",
            "  -f {sam,bam,auto}, --format {sam,bam,auto}\n",
            "                        Type of <alignment_file> data. DEPRECATED: file format\n",
            "                        is detected automatically. This option is ignored.\n",
            "  -r {pos,name}, --order {pos,name}\n",
            "                        'pos' or 'name'. Sorting order of <alignment_file>\n",
            "                        (default: name). Paired-end sequencing data must be\n",
            "                        sorted either by position or by read name, and the\n",
            "                        sorting order must be specified. Ignored for single-\n",
            "                        end data.\n",
            "  --max-reads-in-buffer MAX_BUFFER_SIZE\n",
            "                        When <alignment_file> is paired end sorted by\n",
            "                        position, allow only so many reads to stay in memory\n",
            "                        until the mates are found (raising this number will\n",
            "                        use more memory). Has no effect for single end or\n",
            "                        paired end sorted by name\n",
            "  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n",
            "                        Whether the data is from a strand-specific assay.\n",
            "                        Specify 'yes', 'no', or 'reverse' (default: yes).\n",
            "                        'reverse' means 'yes' with reversed strand\n",
            "                        interpretation\n",
            "  -a MINAQUAL, --minaqual MINAQUAL\n",
            "                        Skip all reads with MAPQ alignment quality lower than\n",
            "                        the given minimum value (default: 10). MAPQ is the 5th\n",
            "                        column of a SAM/BAM file and its usage depends on the\n",
            "                        software used to map the reads.\n",
            "  -t FEATURE_TYPE, --type FEATURE_TYPE\n",
            "                        Feature type (3rd column in GTF file) to be used, all\n",
            "                        features of other type are ignored (default, suitable\n",
            "                        forEnsembl GTF files: exon). If you can call this\n",
            "                        option multiple times, features of all specified types\n",
            "                        will be included, e.g. to include both genes and\n",
            "                        pseudogenes you might use -t gene -t pseudogene.\n",
            "                        Calling this option multiple times is a rare need and\n",
            "                        might result in excessive numbers of ambiguous counts:\n",
            "                        only use if you know what you are doing.\n",
            "  -i IDATTR, --idattr IDATTR\n",
            "                        GTF attribute to be used as feature ID (default,\n",
            "                        suitable for Ensembl GTF files: gene_id). All feature\n",
            "                        of the right type (see -t option) within the same GTF\n",
            "                        attribute will be added together. The typical way of\n",
            "                        using this option is to count all exonic reads from\n",
            "                        each gene and add the exons but other uses are\n",
            "                        possible as well. You can call this option multiple\n",
            "                        times: in that case, the combination of all attributes\n",
            "                        separated by colons (:) will be used as a unique\n",
            "                        identifier, e.g. for exons you might use -i gene_id -i\n",
            "                        exon_number.\n",
            "  --additional-attr ADDITIONAL_ATTRIBUTES\n",
            "                        Additional feature attributes (default: none, suitable\n",
            "                        for Ensembl GTF files: gene_name). Use multiple times\n",
            "                        for more than one additional attribute. These\n",
            "                        attributes are only used as annotations in the output,\n",
            "                        while the determination of how the counts are added\n",
            "                        together is done based on option -i.\n",
            "  --add-chromosome-info\n",
            "                        Store information about the chromosome of each feature\n",
            "                        as an additional attribute (e.g. colunm in the TSV\n",
            "                        output file).\n",
            "  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n",
            "                        Mode to handle reads overlapping more than one feature\n",
            "                        (choices: union, intersection-strict, intersection-\n",
            "                        nonempty; default: union)\n",
            "  --nonunique {none,all,fraction,random}\n",
            "                        Whether and how to score reads that are not uniquely\n",
            "                        aligned or ambiguously assigned to features (choices:\n",
            "                        none, all, fraction, random; default: none)\n",
            "  --secondary-alignments {score,ignore}\n",
            "                        Whether to score secondary alignments (0x100 flag)\n",
            "  --supplementary-alignments {score,ignore}\n",
            "                        Whether to score supplementary alignments (0x800 flag)\n",
            "  -o SAMOUTS, --samout SAMOUTS\n",
            "                        Write out all SAM alignment records into SAM/BAM files\n",
            "                        (one per input file needed), annotating each line with\n",
            "                        its feature assignment (as an optional field with tag\n",
            "                        'XF'). See the -p option to use BAM instead of SAM.\n",
            "  -p {SAM,BAM,sam,bam}, --samout-format {SAM,BAM,sam,bam}\n",
            "                        Format to use with the --samout option.\n",
            "  -d OUTPUT_DELIMITER, --delimiter OUTPUT_DELIMITER\n",
            "                        Column delimiter in output (default: TAB).\n",
            "  -c OUTPUT_FILENAME, --counts_output OUTPUT_FILENAME\n",
            "                        Filename to output the counts to instead of stdout.\n",
            "  --counts-output-sparse\n",
            "                        Store the counts as a sparse matrix (mtx, h5ad, loom).\n",
            "  --append-output       Append counts output to an existing file instead of\n",
            "                        creating a new one. This option is useful if you have\n",
            "                        already creates a TSV/CSV/similar file with a header\n",
            "                        for your samples (with additional columns for the\n",
            "                        feature name and any additionl attributes) and want to\n",
            "                        fill in the rest of the file.\n",
            "  -n NPROCESSES, --nprocesses NPROCESSES\n",
            "                        Number of parallel CPU processes to use (default: 1).\n",
            "                        This option is useful to process several input files\n",
            "                        at once. Each file will use only 1 CPU. It is\n",
            "                        possible, of course, to split a very large input\n",
            "                        SAM/BAM files into smaller chunks upstream to make use\n",
            "                        of this option.\n",
            "  --feature-query FEATURE_QUERY\n",
            "                        Restrict to features descibed in this expression.\n",
            "                        Currently supports a single kind of expression:\n",
            "                        attribute == \"one attr\" to restrict the GFF to a\n",
            "                        single gene or transcript, e.g. --feature-query\n",
            "                        'gene_name == \"ACTB\"' - notice the single quotes\n",
            "                        around the argument of this option and the double\n",
            "                        quotes around the gene name. Broader queries might\n",
            "                        become available in the future.\n",
            "  -q, --quiet           Suppress progress report\n",
            "  --with-header         Whether to add a column header to the output TSV file\n",
            "                        indicating which column corresponds to which input BAM\n",
            "                        file. Only used if output to console or tsv or csv\n",
            "                        file. Default to False.\n",
            "\n",
            "Written by Simon Anders (sanders@fs.tum.de), European Molecular Biology\n",
            "Laboratory (EMBL), Givanna Putri (g.putri@unsw.edu.au) and Fabio Zanini\n",
            "(fabio.zanini@unsw.edu.au), UNSW Sydney. (c) 2010-2021. Released under the\n",
            "terms of the GNU General Public License v3. Please cite the following paper if\n",
            "you use this script: G. Putri et al. Analysing high-throughput sequencing data\n",
            "in Python with HTSeq 2.0. Bioinformatics (2022).\n",
            "https://doi.org/10.1093/bioinformatics/btac166. Part of the 'HTSeq' framework,\n",
            "version 2.0.9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/gff3/solanum_lycopersicum/Solanum_lycopersicum.SL3.0.61.chr.gff3.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUQSdEoYae-K",
        "outputId": "87cafe38-f5ec-49c0-9c47-811476dc4ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 12:27:08--  https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/gff3/solanum_lycopersicum/Solanum_lycopersicum.SL3.0.61.chr.gff3.gz\n",
            "Resolving ftp.ensemblgenomes.ebi.ac.uk (ftp.ensemblgenomes.ebi.ac.uk)... 193.62.193.161\n",
            "Connecting to ftp.ensemblgenomes.ebi.ac.uk (ftp.ensemblgenomes.ebi.ac.uk)|193.62.193.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6448195 (6.1M) [application/x-gzip]\n",
            "Saving to: â€˜Solanum_lycopersicum.SL3.0.61.chr.gff3.gzâ€™\n",
            "\n",
            "Solanum_lycopersicu 100%[===================>]   6.15M  7.61MB/s    in 0.8s    \n",
            "\n",
            "2025-05-25 12:27:10 (7.61 MB/s) - â€˜Solanum_lycopersicum.SL3.0.61.chr.gff3.gzâ€™ saved [6448195/6448195]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/Solanum_lycopersicum.SL3.0.61.chr.gff3.gz"
      ],
      "metadata": {
        "id": "_hVKd0ZxancZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!htseq-count -f bam -s no sample.sorted.bam /content/tomato.gtf > counts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzaEeCAqViRr",
        "outputId": "8104c63a-20a0-4b2c-fedd-a2d429cbdd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000 GFF lines processed.\n",
            "200000 GFF lines processed.\n",
            "300000 GFF lines processed.\n",
            "376550 GFF lines processed.\n",
            "100000 alignment records processed.\n",
            "200000 alignment records processed.\n",
            "300000 alignment records processed.\n",
            "400000 alignment records processed.\n",
            "500000 alignment records processed.\n",
            "600000 alignment records processed.\n",
            "700000 alignment records processed.\n",
            "800000 alignment records processed.\n",
            "900000 alignment records processed.\n",
            "1000000 alignment records processed.\n",
            "1100000 alignment records processed.\n",
            "1200000 alignment records processed.\n",
            "1300000 alignment records processed.\n",
            "1400000 alignment records processed.\n",
            "1500000 alignment records processed.\n",
            "1600000 alignment records processed.\n",
            "1700000 alignment records processed.\n",
            "1800000 alignment records processed.\n",
            "1900000 alignment records processed.\n",
            "2000000 alignment records processed.\n",
            "2100000 alignment records processed.\n",
            "2200000 alignment records processed.\n",
            "2300000 alignment records processed.\n",
            "2400000 alignment records processed.\n",
            "2500000 alignment records processed.\n",
            "2600000 alignment records processed.\n",
            "2700000 alignment records processed.\n",
            "2800000 alignment records processed.\n",
            "2900000 alignment records processed.\n",
            "3000000 alignment records processed.\n",
            "3100000 alignment records processed.\n",
            "3200000 alignment records processed.\n",
            "3300000 alignment records processed.\n",
            "3400000 alignment records processed.\n",
            "3500000 alignment records processed.\n",
            "3600000 alignment records processed.\n",
            "3700000 alignment records processed.\n",
            "3800000 alignment records processed.\n",
            "3900000 alignment records processed.\n",
            "4000000 alignment records processed.\n",
            "4100000 alignment records processed.\n",
            "4200000 alignment records processed.\n",
            "4300000 alignment records processed.\n",
            "4400000 alignment records processed.\n",
            "4500000 alignment records processed.\n",
            "4600000 alignment records processed.\n",
            "4700000 alignment records processed.\n",
            "4800000 alignment records processed.\n",
            "4900000 alignment records processed.\n",
            "5000000 alignment records processed.\n",
            "5100000 alignment records processed.\n",
            "5200000 alignment records processed.\n",
            "5300000 alignment records processed.\n",
            "5400000 alignment records processed.\n",
            "5500000 alignment records processed.\n",
            "5600000 alignment records processed.\n",
            "5700000 alignment records processed.\n",
            "5800000 alignment records processed.\n",
            "5900000 alignment records processed.\n",
            "6000000 alignment records processed.\n",
            "6100000 alignment records processed.\n",
            "6200000 alignment records processed.\n",
            "6300000 alignment records processed.\n",
            "6400000 alignment records processed.\n",
            "6500000 alignment records processed.\n",
            "6600000 alignment records processed.\n",
            "6700000 alignment records processed.\n",
            "6800000 alignment records processed.\n",
            "6900000 alignment records processed.\n",
            "7000000 alignment records processed.\n",
            "7100000 alignment records processed.\n",
            "7200000 alignment records processed.\n",
            "7300000 alignment records processed.\n",
            "7400000 alignment records processed.\n",
            "7500000 alignment records processed.\n",
            "7600000 alignment records processed.\n",
            "7700000 alignment records processed.\n",
            "7800000 alignment records processed.\n",
            "7900000 alignment records processed.\n",
            "8000000 alignment records processed.\n",
            "8100000 alignment records processed.\n",
            "8200000 alignment records processed.\n",
            "8300000 alignment records processed.\n",
            "8400000 alignment records processed.\n",
            "8500000 alignment records processed.\n",
            "8600000 alignment records processed.\n",
            "8700000 alignment records processed.\n",
            "8800000 alignment records processed.\n",
            "8900000 alignment records processed.\n",
            "9000000 alignment records processed.\n",
            "9100000 alignment records processed.\n",
            "9200000 alignment records processed.\n",
            "9300000 alignment records processed.\n",
            "9400000 alignment records processed.\n",
            "9500000 alignment records processed.\n",
            "9600000 alignment records processed.\n",
            "9700000 alignment records processed.\n",
            "9800000 alignment records processed.\n",
            "9900000 alignment records processed.\n",
            "10000000 alignment records processed.\n",
            "10100000 alignment records processed.\n",
            "10200000 alignment records processed.\n",
            "10300000 alignment records processed.\n",
            "10400000 alignment records processed.\n",
            "10500000 alignment records processed.\n",
            "10600000 alignment records processed.\n",
            "10700000 alignment records processed.\n",
            "10800000 alignment records processed.\n",
            "10900000 alignment records processed.\n",
            "11000000 alignment records processed.\n",
            "11100000 alignment records processed.\n",
            "11200000 alignment records processed.\n",
            "11300000 alignment records processed.\n",
            "11400000 alignment records processed.\n",
            "11500000 alignment records processed.\n",
            "11600000 alignment records processed.\n",
            "11700000 alignment records processed.\n",
            "11800000 alignment records processed.\n",
            "11900000 alignment records processed.\n",
            "12000000 alignment records processed.\n",
            "12100000 alignment records processed.\n",
            "12200000 alignment records processed.\n",
            "12300000 alignment records processed.\n",
            "12400000 alignment records processed.\n",
            "12500000 alignment records processed.\n",
            "12600000 alignment records processed.\n",
            "12700000 alignment records processed.\n",
            "12800000 alignment records processed.\n",
            "12900000 alignment records processed.\n",
            "13000000 alignment records processed.\n",
            "13100000 alignment records processed.\n",
            "13200000 alignment records processed.\n",
            "13300000 alignment records processed.\n",
            "13400000 alignment records processed.\n",
            "13500000 alignment records processed.\n",
            "13600000 alignment records processed.\n",
            "13700000 alignment records processed.\n",
            "13800000 alignment records processed.\n",
            "13900000 alignment records processed.\n",
            "14000000 alignment records processed.\n",
            "14100000 alignment records processed.\n",
            "14200000 alignment records processed.\n",
            "14300000 alignment records processed.\n",
            "14400000 alignment records processed.\n",
            "14500000 alignment records processed.\n",
            "14600000 alignment records processed.\n",
            "14700000 alignment records processed.\n",
            "14800000 alignment records processed.\n",
            "14900000 alignment records processed.\n",
            "15000000 alignment records processed.\n",
            "15100000 alignment records processed.\n",
            "15200000 alignment records processed.\n",
            "15300000 alignment records processed.\n",
            "15400000 alignment records processed.\n",
            "15500000 alignment records processed.\n",
            "15600000 alignment records processed.\n",
            "15700000 alignment records processed.\n",
            "15800000 alignment records processed.\n",
            "15900000 alignment records processed.\n",
            "16000000 alignment records processed.\n",
            "16100000 alignment records processed.\n",
            "16200000 alignment records processed.\n",
            "16300000 alignment records processed.\n",
            "16400000 alignment records processed.\n",
            "16500000 alignment records processed.\n",
            "16600000 alignment records processed.\n",
            "16700000 alignment records processed.\n",
            "16800000 alignment records processed.\n",
            "16900000 alignment records processed.\n",
            "17000000 alignment records processed.\n",
            "17100000 alignment records processed.\n",
            "17200000 alignment records processed.\n",
            "17300000 alignment records processed.\n",
            "17400000 alignment records processed.\n",
            "17500000 alignment records processed.\n",
            "17600000 alignment records processed.\n",
            "17615339 alignment records processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!htseq-count \\\n",
        "  -f bam \\\n",
        "  -s no \\               # Strandedness: adjust if needed ('yes' or 'reverse')\n",
        "  -r pos \\              # BAM is coordinate-sorted\n",
        "  -t exon \\             # Count exons (common for GFF3)\n",
        "  -i Parent \\           # Use \"Parent\" to link exons to genes\n",
        "  sample.sorted.bam \\\n",
        "  Solanum_lycopersicum.SL3.0.61.chr.gff3 \\\n",
        "  > counts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "a2AN0Ztab7NC",
        "outputId": "f52d5cda-2e4a-4f6b-9190-c6b755bc0c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-89-8ab41a901e26>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-8ab41a901e26>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -r pos \\              # BAM is coordinate-sorted\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the GFF3 structure (e.g., for gene/exon entries):\n",
        "!zcat /content/Solanum_lycopersicum.SL3.0.61.chr.gff3 | grep -E \"\\tgene\\t|\\texon\\t|\\tmRNA\\t\" | head -n 5 | cut -f9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WS1DFbucV5a",
        "outputId": "0db11bbd-86ef-4317-9b1a-9b81284b62ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: /content/tomato.gtf: not in gzip format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gffread\n",
        "!apt-get install gffread\n",
        "\n",
        "# Convert GFF3 â†’ GTF\n",
        "!gffread Solanum_lycopersicum.SL3.0.61.chr.gff3 -T -o tomato.gtf\n",
        "\n",
        "# Now run htseq-count with the GTF\n",
        "!htseq-count \\\n",
        "  -f bam \\\n",
        "  -s no \\\n",
        "  -r pos \\\n",
        "  tomato.sorted.bam \\\n",
        "  tomato.gtf \\\n",
        "  > counts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbYQz51bc6Yy",
        "outputId": "d3b0e05e-d266-4e32-93c8-955fbd27a96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libgclib3\n",
            "The following NEW packages will be installed:\n",
            "  gffread libgclib3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 93 not upgraded.\n",
            "Need to get 230 kB of archives.\n",
            "After this operation, 678 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgclib3 amd64 0.12.7+ds-4 [162 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gffread amd64 0.12.7-2build1 [67.4 kB]\n",
            "Fetched 230 kB in 1s (313 kB/s)\n",
            "Selecting previously unselected package libgclib3:amd64.\n",
            "(Reading database ... 127241 files and directories currently installed.)\n",
            "Preparing to unpack .../libgclib3_0.12.7+ds-4_amd64.deb ...\n",
            "Unpacking libgclib3:amd64 (0.12.7+ds-4) ...\n",
            "Selecting previously unselected package gffread.\n",
            "Preparing to unpack .../gffread_0.12.7-2build1_amd64.deb ...\n",
            "Unpacking gffread (0.12.7-2build1) ...\n",
            "Setting up libgclib3:amd64 (0.12.7+ds-4) ...\n",
            "Setting up gffread (0.12.7-2build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/liblzma.so.5 is not a symbolic link\n",
            "\n",
            "[E::hts_open_format] Failed to open file \"tomato.sorted.bam\" : No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/htseq-count\", line 6, in <module>\n",
            "    HTSeq.scripts.count.main()\n",
            "  File \"/usr/local/lib/python3.11/site-packages/HTSeq/scripts/count.py\", line 509, in main\n",
            "    args = _parse_sanitize_cmdline_arguments()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/HTSeq/scripts/count.py\", line 501, in _parse_sanitize_cmdline_arguments\n",
            "    _check_sam_files(args.samfilenames)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/HTSeq/scripts/count.py\", line 158, in _check_sam_files\n",
            "    with pysam.AlignmentFile(sam_filename, \"r\") as sf:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"pysam/libcalignmentfile.pyx\", line 751, in pysam.libcalignmentfile.AlignmentFile.__cinit__\n",
            "  File \"pysam/libcalignmentfile.pyx\", line 950, in pysam.libcalignmentfile.AlignmentFile._open\n",
            "FileNotFoundError: [Errno 2] could not open alignment file `tomato.sorted.bam`: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}
